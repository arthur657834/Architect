socket的输入需要两个阶段：

1. 等待数据准备好。
2. 从内核向进程复制数据。
因为等待的过程是阻塞式，所以我们上面使用多线程就是降低这个阻塞所带来的影响。

现在来看五种IO模型：
1. 阻塞IO模型
> recv->无数据报准备好->等待数据->数据报准备好->数据从内核复制到用户空间->复制完成->返回成功指示

2. 非阻塞IO模型
> recv->无数据报准备好->返回EWOULDBLOCK->recv->无数据报准备好->返回EWOULDBLOCK->数据报准备好->数据从内核复制到用户空间->复制完成->返回成功指示
特点：轮询操作，大量占用cpu时间。

3. IO复用模型
> select->无数据报准备好->数据报准备好->返回可读条件->recv->数据从内核复制到用户空间->复制完成->返回成功指示

4. 信号驱动模型
> 建立信号处理程序(sigaction)->递交SIGIO->recv->数据从内核复制到用户空间->复制完成->返回成功指示

5. 异步IO模型
> aio_read->无数据准备好->数据报准备好->数据从内核复制到用户空间->复制完成->递交aio_read中指定的信号
> 特点：直到数据复制完成产生信号的过程中进程都不被阻塞。

为了获取更好的性能，我们一般采用IO多路复用模型，例如select和poll操作，运行进程同时检查多个文件描述符以找出它们任意一个是否可以进行IO操作，内核一旦发现进程指定的一个或多个IO条件就绪（输入准备被读取，或描述符能承接更多的输出），它就通知进程。

但前面说了select和poll有一个弊端就是他们在检查可用描述符的时候都是不断地遍历又遍历，当要监听的socket的文件描述符数量庞大时，性能会急剧下降，CPU消耗严重。

信号驱动模型比他们优越的地方在于，当有输入数据来到指定的文件描述符时，内核向请求数据的进程发送一个信号，进程可以处理其他任务，通过接收信号以获得通知。

而epoll则更进一步，用事件驱动的方式来监听fd，避免了信号处理的繁琐，在文件描述符上注册事件函数，由系统监视这些文件描述符，当在文件描述符可就绪时，内核通知应用进程。

在一些高并发的网络操作上，epoll的性能通常比select跟poll好几个数量级。

IO调用中有两个概念：

水平触发：如果文件描述符可以非阻塞地进行io调用，此时认为他已经就绪）。（支持模型：select，poll，epoll等）
边缘触发：如果文件描述符自上次来的时候有了新的io活动（新的输入），触发通知。（支持模型：信号驱动，epoll等）
在实际开发中要注意他们的区别，知道边缘触发为什么可能产生socket饥饿问题，怎么解决。

用一张图总结5个IO模型是这样的：

![socket] (socket.jpg)

select的几大缺点：
1. 每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大
2. 同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大
3. select支持的文件描述符数量太小了，默认是1024

poll的实现和select非常相似，只是描述fd集合的方式不同，poll使用pollfd结构而不是select的fd_set结构，其他的都差不多。

总结：
1. select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。
2. select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。


